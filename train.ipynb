{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Visualize Single Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_file_name = \"./blues.00000.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,sr = librosa.load(random_file_name,sr=50000) ##sr=44100\n",
    "plt.figure(figsize=(14,5))\n",
    "librosa.display.waveshow(y,sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(data=y,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing Visualization on chunks of Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"./blues.00000.wav\"\n",
    "y,sr = librosa.load(random_file_name,sr=None) #sr=None means original sampling rate\n",
    "\n",
    "#define duration of chunk and overlap\n",
    "chunk_duration = 4\n",
    "overlap_duration = 2\n",
    "\n",
    "#Convert duartion to sample\n",
    "chunk_samples = chunk_duration * sr\n",
    "overlap_samples = overlap_duration * sr\n",
    "\n",
    "\n",
    "#Calculate the number of chunks\n",
    "num_chunks = int(np.ceil((len(y)-chunk_samples)/(chunk_samples-overlap_samples)))+1\n",
    "\n",
    "#Iterate over each chunk\n",
    "\n",
    "for i in range (num_chunks):\n",
    "    #Calculate start and end indices of each chunk\n",
    "    start = i*(chunk_samples-overlap_samples)\n",
    "    end = start + chunk_samples\n",
    "    #Extract the chunk Audio\n",
    "    chunk = y[start:end]\n",
    "    plt.Figure(figsize=(4,2))\n",
    "    librosa.display.waveshow(chunk,sr=sr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melspectogram Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting  Melspectogram of entire Audio\n",
    "def plot_melspectogram(y,sr):\n",
    "    #Compute Spectogram\n",
    "    spectogram = librosa.feature.melspectrogram(y=y,sr=sr)\n",
    "    #Convert to decibles (log scale)\n",
    "    spectogram_db = librosa.power_to_db(spectogram, ref=np.max)\n",
    "    #Visualize spectogram\n",
    "    plt.figure(figsize=(10,4))\n",
    "    librosa.display.specshow(spectogram_db,sr=sr,x_axis='time',y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Melspectogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_file_name = \"./blues.00000.wav\"\n",
    "y,sr = librosa.load(random_file_name,sr=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For entire Audio\n",
    "plot_melspectogram(y,sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_melspectogram_chunks(y,sr):\n",
    "    audio_path = \"./blues.00000.wav\"\n",
    "y,sr = librosa.load(random_file_name,sr=None) #sr=None means original sampling rate\n",
    "\n",
    "#define duration of chunk and overlap\n",
    "chunk_duration = 4\n",
    "overlap_duration = 2\n",
    "\n",
    "#Convert duartion to sample\n",
    "chunk_samples = chunk_duration * sr\n",
    "overlap_samples = overlap_duration * sr\n",
    "\n",
    "\n",
    "#Calculate the number of chunks\n",
    "num_chunks = int(np.ceil((len(y)-chunk_samples)/(chunk_samples-overlap_samples)))+1\n",
    "\n",
    "#Iterate over each chunk\n",
    "\n",
    "for i in range (num_chunks):\n",
    "    #Calculate start and end indices of each chunk\n",
    "    start = i*(chunk_samples-overlap_samples)\n",
    "    end = start + chunk_samples\n",
    "    #Extract the chunk Audio\n",
    "    chunk = y[start:end]\n",
    "    #Melspectogram of each chunk\n",
    "    melspectogram = librosa.feature.melspectrogram(y=chunk,sr=sr)\n",
    "    spectogram_db = librosa.power_to_db(melspectogram, ref=np.max)\n",
    "    #visualize melspectogram\n",
    "    plt.figure(figsize=(10,4))\n",
    "    librosa.display.specshow(spectogram_db,sr=sr,x_axis='time',y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Melspectogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_file_name=\"./blues.00000.wav\"\n",
    "y,sr = librosa.load(random_file_name,sr=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_melspectogram_chunks(y,sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./genres_original\"\n",
    "classes = ['blues', 'classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.image import resize\n",
    "#Load and preprocess audio data\n",
    "def load_and_preprocess_data(data_dir,classes,target_shape=(150,150)):\n",
    "    data=[]\n",
    "    labels=[]\n",
    "\n",
    "    for i_class,class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir,class_name)\n",
    "        print(\"Processing--\",class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith('.wav'):\n",
    "                file_path = os.path.join(class_dir,filename)\n",
    "                audio_data,sample_rate = librosa.load(file_path,sr=None)\n",
    "                #Performing Preprocessing\n",
    "                #define the duration of each chunk and overlap\n",
    "                chunk_duration = 4\n",
    "                overlap_duration = 2\n",
    "                \n",
    "                #Convert duration to sample\n",
    "                chunk_samples = chunk_duration * sample_rate\n",
    "                overlap_samples = overlap_duration * sample_rate\n",
    "                \n",
    "                #Calculate the number of chunks\n",
    "                num_chunks = int(np.ceil((len(audio_data)-chunk_samples)/(chunk_samples-overlap_samples)))+1\n",
    "                \n",
    "                #iterate over each chunks\n",
    "                for i in range(num_chunks):\n",
    "                    #Calculate start and end indices of the chunk\n",
    "                    start = i*(chunk_samples-overlap_samples)\n",
    "                    end = start+chunk_samples\n",
    "                    #Extract the chunk audio\n",
    "                    chunk = audio_data[start:end]\n",
    "                    #Melspectrogram part\n",
    "                    mel_spectrogram = librosa.feature.melspectrogram(y=chunk,sr=sample_rate)\n",
    "                    #Resize matrix based on provided target shape\n",
    "                    mel_spectrogram = resize(np.expand_dims(mel_spectrogram,axis=-1),target_shape)\n",
    "                    #Append data to list\n",
    "                    data.append(mel_spectrogram)\n",
    "                    labels.append(i_class)\n",
    "    #Return\n",
    "    return np.array(data),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,labels = load_and_preprocess_data(data_dir,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "labels = to_categorical(labels,num_classes = len(classes)) # Converting labels to one-hot encoding\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(data,labels,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',input_shape=X_train[0].shape))\n",
    "model.add(Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=64,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=128,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=256,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=512,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=512,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=1200,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=len(classes),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = model.fit(X_train,Y_train,epochs=30,batch_size=32,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Trained_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
